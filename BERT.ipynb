{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOg3iEKQGosmbON7S0Kmbea"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SQwVoY75D-K1"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","mug_csv = pd.read_csv('mug.csv')\n","kettle_csv = pd.read_csv('kettle.csv')\n","bottle_csv = pd.read_csv('bottle.csv')\n","bakingtray_csv = pd.read_csv('bakingtray.csv')\n","hammer_csv = pd.read_csv('hammer.csv')\n","pan_csv = pd.read_csv('pan.csv')\n","wrench_csv = pd.read_csv('wrench.csv')\n","\n","\n","mug_csv['Object'] = ['mug'] * len(mug_csv)\n","kettle_csv['Object'] = ['kettle'] * len(kettle_csv)\n","bottle_csv['Object'] = ['bottle'] * len(bottle_csv)\n","bakingtray_csv['Object'] = ['bakingtray'] * len(bakingtray_csv)\n","hammer_csv['Object'] = ['hammer'] * len(hammer_csv)\n","pan_csv['Object'] = ['pan'] * len(pan_csv)\n","wrench_csv['Object'] = ['wrench'] * len(wrench_csv)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjkPj4fLEII5"},"source":["all_df = mug_csv.append(kettle_csv, ignore_index=True)\n","all_df = all_df.append(bottle_csv, ignore_index=True)\n","all_df = all_df.append(bakingtray_csv, ignore_index=True)\n","all_df = all_df.append(hammer_csv, ignore_index=True)\n","all_df = all_df.append(pan_csv, ignore_index=True)\n","all_df = all_df.append(wrench_csv, ignore_index=True)\n","\n","print(len(all_df))\n","all_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHA2BEL0D5D9"},"source":["import string # for preprocess_text()\n","import re\n","# NLTK library to remove the stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","'''\n","Want to try to method\n","1) change the digit to word\n","2) split the number into single digit\n","'''\n","num_to_word = {'1' : ' one ',\n","               '2' : ' two ',\n","               '3' : ' three ',\n","               '4' : ' four ',\n","               '5' : ' five ',\n","               '6' : ' six ',\n","               '7' : ' eight ',\n","               '8' : ' eight ',\n","               '9' : ' nine ',\n","               '0' : ' zero '}\n","\n","regex = '([0-9]+\\.?[0-9]+)([a-zA-Z]+)'\n","r = re.compile(regex)\n","\n","# unit conversion\n","convert_vol = {'oz'     : 29.5735,\n","               'cl'     : 10.0,\n","               'gallon' : 4546.09,\n","               'L'      : 1000.0}\n","char_set = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n","\n","class InputPreprocess():\n","  @staticmethod\n","  def quantize(val, to_values):\n","      \"\"\"Quantize a value with regards to a set of allowed values.\n","      \n","      Examples:\n","          quantize(49.513, [0, 45, 90]) -> 45\n","          quantize(43, [0, 10, 20, 30]) -> 30\n","      \n","      Note: function doesn't assume to_values to be sorted and\n","      iterates over all values (i.e. is rather slow).\n","      \n","      Args:\n","          val        The value to quantize\n","          to_values  The allowed values\n","      Returns:\n","          Closest value among allowed values.\n","      \"\"\"\n","      best_match = None\n","      best_match_diff = None\n","      for other_val in to_values:\n","          diff = abs(other_val - val)\n","          if best_match is None or diff <= best_match_diff:\n","              best_match = other_val\n","              best_match_diff = diff\n","      return best_match\n","\n","  # Preprocess weights --> e.g. 990g -> 990 <int>\n","  @staticmethod\n","  def preprocess_weights(df):\n","    tmp = df.copy()\n","    weights = tmp['Weight'].to_list()\n","    # Want to extract rows with 'g' only\n","    row_g = []\n","    for i in range(len(weights)):\n","      curr_weight = weights[i]\n","      if curr_weight.endswith('kg'):\n","        curr_weight = curr_weight[0:-2]\n","        curr_weight = float(curr_weight) * 1000\n","        # row_g.append(i)\n","      elif curr_weight.endswith('g'):\n","        curr_weight = curr_weight[0:-1]\n","        curr_weight = float(curr_weight)\n","        row_g.append(i)\n","      elif curr_weight.endswith('pounds'):\n","        curr_weight = curr_weight[0:-6]\n","        curr_weight = float(curr_weight) * 453.592\n","      elif curr_weight.endswith('oz'):\n","        curr_weight = curr_weight[0:-2]\n","        curr_weight = float(curr_weight) * 28.3495\n","      elif curr_weight == 'other':\n","        curr_weight = 0\n","      if curr_weight > 5000:\n","        curr_weight = 0\n","      weights[i] = int(curr_weight)\n","    tmp['Weight'] = weights\n","    return tmp, row_g\n","  \n","  @staticmethod\n","  def preprocess_volume(df):\n","    tmp = df.copy()\n","    volume = tmp['Volume'].to_list()\n","    for i in range(len(volume)):\n","      curr_volume = volume[i].split(';')[0] # now only consider to first element in the labels\n","\n","      if curr_volume.endswith('ml'):\n","        curr_volume = float(curr_volume[0:-2])\n","      elif curr_volume.endswith('oz'):\n","        curr_volume = float(curr_volume[0:-2]) * convert_vol['oz']\n","      elif curr_volume.endswith('g'):\n","        curr_volume = float(curr_volume[0:-1])\n","      elif curr_volume.endswith('cl'):\n","        curr_volume = float(curr_volume[0:-2]) * convert_vol['cl']\n","      elif curr_volume.endswith('L'):\n","        curr_volume = float(curr_volume[0:-1]) * convert_vol['L']\n","      elif curr_volume.endswith('gallon'):\n","        curr_volume = float(curr_volume[0:-6]) * convert_vol['gallon']\n","      elif curr_volume == 'other':\n","        curr_volume = 0\n","      volume[i] = int(curr_volume)\n","      # if volume[i] > 5000:\n","      #   volume[i] = 0\n","    tmp['Volume'] = volume\n","    return tmp\n","\n","  @staticmethod\n","  def convert_num2words(word_list):\n","    tmp_word_list = []\n","    for word in word_list:\n","      if word[0].isdigit() and word[-1].isdigit():\n","        try:\n","          num_in_word = num2words(float(word))\n","          tmp_word_list.append(num_in_word)\n","        except ValueError as e:\n","          tmp_word_list.append(word)\n","      else:\n","        tmp_word_list.append(word)\n","    return tmp_word_list\n","  @staticmethod\n","  def separate_num_unit(word_list):\n","    tmp_word_list = []\n","    for word in word_list:\n","      if word == '':\n","        continue\n","      m = r.match(word)\n","      if m is not None:\n","        tmp_word_list.extend(list(m.groups()))\n","      else:\n","        tmp_word_list.append(word)\n","    return tmp_word_list\n","\n","  @staticmethod\n","  def remove_stopwords(text):\n","    words = [w for w in text if w not in stopwords.words('english')]\n","    return words\n","  \n","  @staticmethod\n","  def remove_out_of_char(sentence):\n","    res = ''\n","    for ch in sentence:\n","      if ch in char_set:\n","        res += ch\n","      else:\n","        res += ' '\n","    return res\n","  \"\"\"\n","  Clean the text with the following rules\n","  - Convert newline \\n to white space\n","  - Convert tab \\t to white space\n","  - Lowercase all texts\n","  - Covert punctuation to white space\n","\n","  \"\"\"\n","  @staticmethod\n","  def preprocess_text(df, column):\n","    tmp = df.copy()\n","    for i in range(len(tmp)):\n","      s = tmp[column][i]\n","      s = s.lower()\n","      s = s.replace('\\n', ' ') \n","      s = s.replace('\\t', ' ')\n","      s = InputPreprocess.remove_out_of_char(s)\n","      word_list = s.split(' ')\n","      # handle cases like 12oz -> 12 oz (separate the value and the unit)\n","      tmp_word_list = InputPreprocess.separate_num_unit(word_list)\n","\n","      # word_list = InputPreprocess.convert_num2words(tmp_word_list)\n","      s = ' '.join(tmp_word_list)\n","      res_string = ''\n","      for j in range(len(s)):\n","        if s[j] == '.' and j-1 > 0 and j+1 < len(s) and s[j-1].isdigit() and s[j+1].isdigit():\n","          res_string += s[j]\n","        elif s[j] in string.punctuation:\n","          continue\n","        else:\n","          res_string += s[j]\n","\n","      # s = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in s]).split())\n","      tmp.at[i, column] = res_string.split(' ')\n","    # Remove stopwords using NLTK library\n","    print('applying NLTK')\n","    tmp[column] = tmp[column].apply(lambda x: InputPreprocess.remove_stopwords(x))\n","    for i in range(len(tmp)):\n","      s = tmp[column][i]\n","      s = ' '.join(s)\n","      tmp.at[i, column] = s\n","    return tmp\n","  \n","  @staticmethod\n","  def preprocess_dimensions(df):\n","    tmp = df.copy()\n","    length, width, height = tmp['Length'], tmp['Width'], tmp['Height']\n","    res_length, res_width, res_height = [], [], []\n","    for i in range(len(df)):\n","      if length[i] == 'other':\n","        res_length.append(0)\n","      else:\n","        res_length.append(int(round(float(length[i]))))\n","\n","      if width[i] == 'other':\n","        res_width.append(0)\n","      else:\n","        res_width.append(int(round(float(width[i]))))\n","\n","      if height[i] == 'other':\n","        res_height.append(0)\n","      else:\n","        res_height.append(int(round(float(height[i]))))\n","    tmp['Length'], tmp['Width'], tmp['Height'] = res_length, res_width, res_height\n","    return tmp\n","\n","  @staticmethod\n","  def parts_into_list(df):\n","    tmp = df.copy()\n","    for i in range(len(tmp)):\n","      s = tmp['Parts'][i]\n","      s = s.split(';')\n","      tmp.at[i, 'Parts'] = s\n","    return tmp    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c5qgzt2rEb0w"},"source":["'''\n","NOTE: Change this if needed for numeric label resolution\n","'''\n","weight_vol_res = 100\n","size_res = 3\n","\n","# Pre-process on input text\n","obj_df = InputPreprocess.preprocess_text(all_df, 'Input')\n","obj_df = InputPreprocess.preprocess_text(obj_df, 'UnstructuredText')\n","obj_df = InputPreprocess.preprocess_text(obj_df, 'StructuredText')\n","\n","# Pre-process on weight and volume\n","obj_df, row_g = InputPreprocess.preprocess_weights(obj_df)\n","obj_df = InputPreprocess.preprocess_volume(obj_df)\n","weight_bins = np.arange(min(obj_df['Weight']), max(obj_df['Weight']), weight_vol_res)\n","volume_bins = np.arange(min(obj_df['Volume']), max(obj_df['Volume']), weight_vol_res)\n","obj_df['Weight'] = obj_df['Weight'].apply(lambda x : InputPreprocess.quantize(x, weight_bins))\n","obj_df['Volume'] = obj_df['Volume'].apply(lambda x : InputPreprocess.quantize(x, volume_bins))\n","\n","# Pre-process on the size\n","obj_df = InputPreprocess.preprocess_dimensions(obj_df)\n","length_bins = np.arange(min(obj_df['Length']), max(obj_df['Length']), size_res)\n","width_bins = np.arange(min(obj_df['Width']), max(obj_df['Width']), size_res)\n","height_bins = np.arange(min(obj_df['Height']), max(obj_df['Height']), size_res)\n","obj_df['Length'] = obj_df['Length'].apply(lambda x : InputPreprocess.quantize(x, length_bins))\n","obj_df['Width'] = obj_df['Width'].apply(lambda x : InputPreprocess.quantize(x, width_bins))\n","obj_df['Height'] = obj_df['Height'].apply(lambda x : InputPreprocess.quantize(x, height_bins))\n","\n","# Pre-process on the object parts\n","obj_df = InputPreprocess.parts_into_list(obj_df)\n","obj_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7aZueXqOWXEm"},"source":["# For object parts we cannot use one-hot encoding, have to use a multi-label binarizer\n","from sklearn.preprocessing import MultiLabelBinarizer\n","mlb = MultiLabelBinarizer()\n","obj_df['Parts'] = list(mlb.fit_transform(list(obj_df['Parts'])))\n","obj_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZvp_h-kGWMh"},"source":["material_list = list(set(obj_df[\"Material\"]))\n","print(len(material_list), material_list)\n","\n","colour_list = list(set(obj_df['Colour']))\n","print(len(colour_list), colour_list)\n","\n","weight_list = list(set(obj_df['Weight']))\n","print(len(weight_list), sorted(weight_list))\n","\n","volume_list = list(set(obj_df['Volume']))\n","print(len(volume_list), sorted(volume_list))\n","\n","object_list = list(set(obj_df['Object']))\n","print(len(object_list), sorted(object_list))\n","\n","length_list = list(set(obj_df['Length']))\n","print(len(length_list), sorted(length_list))\n","\n","width_list = list(set(obj_df['Width']))\n","print(len(width_list), sorted(width_list))\n","\n","height_list = list(set(obj_df['Height']))\n","print(len(height_list), sorted(height_list))\n","\n","part_list = list(mlb.classes_)\n","print(len(part_list), part_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jlgs7fWYGXl7"},"source":["# Create the dictionary for the properties\n","material_to_idx = {x : i for i, x in enumerate(material_list)}\n","colour_to_idx = {x : i for i, x in enumerate(colour_list)}\n","weight_to_idx = {x : i for i, x in enumerate(weight_list)}\n","volume_to_idx = {x : i for i, x in enumerate(volume_list)}\n","object_to_idx = {x : i for i, x in enumerate(object_list)}\n","length_to_idx = {x : i for i, x in enumerate(length_list)}\n","width_to_idx = {x : i for i, x in enumerate(width_list)}\n","height_to_idx = {x : i for i, x in enumerate(height_list)}\n","# Now update the label to integer form\n","for i in range(len(obj_df)):\n","  obj_df.iloc[i, 3] = material_to_idx[obj_df.iloc[i, 3]]\n","  obj_df.iloc[i, 4] = colour_to_idx[obj_df.iloc[i, 4]]\n","  obj_df.iloc[i, 5] = weight_to_idx[obj_df.iloc[i, 5]]\n","  obj_df.iloc[i, 6] = volume_to_idx[obj_df.iloc[i, 6]]\n","  obj_df.iloc[i, 7] = length_to_idx[obj_df.iloc[i, 7]]\n","  obj_df.iloc[i, 8] = width_to_idx[obj_df.iloc[i, 8]]\n","  obj_df.iloc[i, 9] = height_to_idx[obj_df.iloc[i, 9]]\n","  obj_df.iloc[i, -1] = object_to_idx[obj_df.iloc[i, -1]]\n","obj_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbnVlm12WLcS"},"source":["# Plot the input sentence length\n","import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")\n","%matplotlib inline\n","fig = plt.figure(figsize=(15, 3), dpi=200)\n","ax1 = fig.add_subplot(1, 3, 1)\n","ax2 = fig.add_subplot(1, 3, 2)\n","ax3 = fig.add_subplot(1, 3, 3)\n","ax1.hist([len(sen.split(' ')) for sen in obj_df[\"Input\"]], bins=50)\n","ax2.hist([len(sen.split(' ')) for sen in obj_df[\"UnstructuredText\"]], bins=50)\n","ax3.hist([len(sen.split(' ')) for sen in obj_df[\"StructuredText\"]], bins=50)\n","\n","ax1.set_xlabel('Length of sentence')\n","ax1.set_ylabel('Number of sentences')\n","ax1.set_title('All Text')\n","\n","ax2.set_xlabel('Length of sentence')\n","ax2.set_ylabel('Number of sentences')\n","ax2.set_title('Unstructured Text')\n","\n","ax3.set_xlabel('Length of sentence')\n","ax3.set_ylabel('Number of sentences')\n","ax3.set_title('Structured Text')\n","# plt.savefig('Dataset_distribution.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1ypVjrVWpaX"},"source":["# Need to truncate the input otherwise out of memeory problem\n","def truncate_input(df, column, max_len=250): #only want the last 250 words\n","  tmp = df.copy()\n","  for i in range(len(tmp)):\n","    s = tmp[column][i]\n","    s = s.split(' ')\n","    if len(s) > max_len:\n","      s = s[-1*max_len:]\n","    s = ' '.join(s)\n","    tmp.at[i, column] = s\n","  return tmp\n","obj_df = truncate_input(obj_df, 'Input', 200)\n","obj_df = truncate_input(obj_df, 'StructuredText', 100)\n","obj_df = truncate_input(obj_df, 'UnstructuredText', 150)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cx_AYBX77B54"},"source":["### Trying BERT from the following link\n","https://www.analyticsvidhya.com/blog/2020/10/simple-text-multi-classification-task-using-keras-bert/"]},{"cell_type":"code","metadata":{"id":"ayJM4rJb6326"},"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import logging\n","logging.basicConfig(level=logging.INFO)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o042zZh_6_4R"},"source":["!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDmCzcl47Jy8"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bd-LQkg37Bag"},"source":["import tensorflow_hub as hub\n","import tokenization\n","module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n","bert_layer = hub.KerasLayer(module_url, trainable=True)\n","struct_bert_layer = hub.KerasLayer(module_url, trainable=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSWs-SXG7IOX"},"source":["vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n","\n","def bert_encode(texts, tokenizer, max_len=512):\n","    all_tokens = []\n","    all_masks = []\n","    all_segments = []\n","    \n","    for text in texts:\n","        text = tokenizer.tokenize(text)\n","            \n","        text = text[:max_len-2]\n","        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n","        pad_len = max_len - len(input_sequence)\n","        \n","        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n","        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n","        segment_ids = [0] * max_len\n","        \n","        all_tokens.append(tokens)\n","        all_masks.append(pad_masks)\n","        all_segments.append(segment_ids)\n","    \n","    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qxFUq3a_TP_2"},"source":["# Run this for single output model"]},{"cell_type":"code","metadata":{"id":"P8CeHimj7RS1"},"source":["def build_model(bert_layer, prop_list, max_len=512):\n","    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","\n","    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    clf_output = sequence_output[:, 0, :]\n","    net = tf.keras.layers.Dense(256, activation='relu')(clf_output)\n","    net = tf.keras.layers.Dropout(0.2)(net)\n","    net = tf.keras.layers.Dense(256, activation='relu')(net)\n","    net = tf.keras.layers.Dropout(0.2)(net)\n","    out = tf.keras.layers.Dense(len(prop_list), activation='softmax')(net)\n","    \n","    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n","    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WB4czjumT4zY"},"source":["# Run this instead for multiple output model"]},{"cell_type":"code","metadata":{"id":"tl_4MTu-AKUc"},"source":["# Multiple output\n","def build_model(bert_layer, max_len=512):\n","    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","\n","    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    clf_output = sequence_output[:, 0, :]\n","    net = tf.keras.layers.Dense(256, activation='relu')(clf_output)\n","    net = tf.keras.layers.Dropout(0.2)(net)\n","\n","    # Material branch\n","    material_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    material_branch = tf.keras.layers.Dropout(0.2)(material_branch)\n","    material_branch = tf.keras.layers.Dense(len(material_list), activation='softmax', name='material_output')(material_branch)\n","    # Colour branch\n","    colour_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    colour_branch = tf.keras.layers.Dropout(0.2)(colour_branch)\n","    colour_branch = tf.keras.layers.Dense(len(colour_list), activation='softmax', name='colour_output')(colour_branch)\n","    # Weight branch\n","    weight_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    weight_branch = tf.keras.layers.Dropout(0.2)(weight_branch)\n","    weight_branch = tf.keras.layers.Dense(len(weight_list), activation='softmax', name='weight_output')(weight_branch)\n","    # Volume branch\n","    volume_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    volume_branch = tf.keras.layers.Dropout(0.2)(volume_branch)\n","    volume_branch = tf.keras.layers.Dense(len(volume_list), activation='softmax', name='volume_output')(volume_branch)\n","    # Object Branch\n","    object_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    object_branch = tf.keras.layers.Dropout(0.2)(object_branch)\n","    object_branch = tf.keras.layers.Dense(len(object_list), activation='softmax', name='object_output')(object_branch)\n","    # Length Branch\n","    length_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    length_branch = tf.keras.layers.Dropout(0.2)(length_branch)\n","    length_branch = tf.keras.layers.Dense(len(length_list), activation='softmax', name='length_output')(length_branch)\n","    # Width Branch\n","    width_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    width_branch = tf.keras.layers.Dropout(0.2)(width_branch)\n","    width_branch = tf.keras.layers.Dense(len(width_list), activation='softmax', name='width_output')(width_branch)\n","    # Height Branch\n","    height_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    height_branch = tf.keras.layers.Dropout(0.2)(height_branch)\n","    height_branch = tf.keras.layers.Dense(len(height_list), activation='softmax', name='height_output')(height_branch)\n","    # Part branch\n","    part_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    part_branch = tf.keras.layers.Dropout(0.2)(part_branch)\n","    part_branch = tf.keras.layers.Dense(len(part_list), activation='sigmoid', name='part_output')(part_branch)\n","\n","    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[material_branch, colour_branch, weight_branch, volume_branch, object_branch, length_branch, width_branch, height_branch, part_branch])    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"di1Wj3XNT7zd"},"source":["# This was used to test if separately the input as structured and unstructured would do any good"]},{"cell_type":"code","metadata":{"id":"KhIX-Mh1_gPV"},"source":["# Multiple output\n","def build_multi_input_model(bert_layer, struct_bert_layer, struct_max_len, max_len):\n","  # Unstructured text input\n","    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","    # structured text input\n","    struct_input_word_ids = tf.keras.Input(shape=(struct_max_len,), dtype=tf.int32, name=\"struct_input_word_ids\")\n","    struct_input_mask = tf.keras.Input(shape=(struct_max_len,), dtype=tf.int32, name=\"struct_input_mask\")\n","    struct_segment_ids = tf.keras.Input(shape=(struct_max_len,), dtype=tf.int32, name=\"struct_segment_ids\")\n","\n","    struct_pooled_output, struct_sequence_output = struct_bert_layer([struct_input_word_ids, struct_input_mask, struct_segment_ids])\n","    struct_clf_output = struct_sequence_output[:, 0, :]\n","    struct_net = tf.keras.layers.Dense(256, activation='relu')(struct_clf_output)\n","    struct_net = tf.keras.layers.Dropout(0.2)(struct_net)\n","\n","\n","    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    clf_output = sequence_output[:, 0, :]\n","    net = tf.keras.layers.Dense(256, activation='relu')(clf_output)\n","    net = tf.keras.layers.Dropout(0.2)(net)\n","\n","    net = tf.keras.layers.concatenate([struct_net, net])\n","\n","    # Material branch\n","    material_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    material_branch = tf.keras.layers.Dropout(0.2)(material_branch)\n","    material_branch = tf.keras.layers.Dense(len(material_list), activation='softmax', name='material_output')(material_branch)\n","    # Colour branch\n","    colour_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    colour_branch = tf.keras.layers.Dropout(0.2)(colour_branch)\n","    colour_branch = tf.keras.layers.Dense(len(colour_list), activation='softmax', name='colour_output')(colour_branch)\n","    # Weight branch\n","    weight_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    weight_branch = tf.keras.layers.Dropout(0.2)(weight_branch)\n","    weight_branch = tf.keras.layers.Dense(len(weight_list), activation='softmax', name='weight_output')(weight_branch)\n","    # Volume branch\n","    volume_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    volume_branch = tf.keras.layers.Dropout(0.2)(volume_branch)\n","    volume_branch = tf.keras.layers.Dense(len(volume_list), activation='softmax', name='volume_output')(volume_branch)\n","    # Object Branch\n","    object_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    object_branch = tf.keras.layers.Dropout(0.2)(object_branch)\n","    object_branch = tf.keras.layers.Dense(len(object_list), activation='softmax', name='object_output')(object_branch)\n","    # Length Branch\n","    length_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    length_branch = tf.keras.layers.Dropout(0.2)(length_branch)\n","    length_branch = tf.keras.layers.Dense(len(length_list), activation='softmax', name='length_output')(length_branch)\n","    # Width Branch\n","    width_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    width_branch = tf.keras.layers.Dropout(0.2)(width_branch)\n","    width_branch = tf.keras.layers.Dense(len(width_list), activation='softmax', name='width_output')(width_branch)\n","    # Height Branch\n","    height_branch = tf.keras.layers.Dense(256, activation='relu')(net)\n","    height_branch = tf.keras.layers.Dropout(0.2)(height_branch)\n","    height_branch = tf.keras.layers.Dense(len(height_list), activation='softmax', name='height_output')(height_branch)\n","\n","\n","\n","    model = tf.keras.models.Model(inputs=[[input_word_ids, input_mask, segment_ids], [struct_input_word_ids, struct_input_mask, struct_segment_ids]], outputs=[material_branch, colour_branch, weight_branch, volume_branch, object_branch, length_branch, width_branch, height_branch])    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9bcjGkBT3L7"},"source":["def max_sequence_length(df, column):\n","  tmp = df.copy()\n","  seq_length = []\n","  for i in range(len(df)):\n","    seq_length.append(len(df[column][i].split(' ')))\n","  print(f'Max sequence length (by words) in {column} is {max(seq_length)}')\n","max_sequence_length(obj_df, 'Input')\n","max_sequence_length(obj_df, 'StructuredText')\n","max_sequence_length(obj_df, 'UnstructuredText')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfdStoXj7YP9"},"source":["'''\n","Desired input\n","change it to Input/StructuredText/UnstructuredText if needed\n","'''\n","text_type = 'StructuredText' \n","'''\n","max_len = 100 for structured text\n","max_len = 200/250 for unstructured text\n","'''\n","max_len = 100\n","struct_max_len = 100\n","\n","\n","inputs = bert_encode(obj_df[text_type], tokenizer, max_len=max_len)\n","struct_inputs = bert_encode(obj_df['StructuredText'], tokenizer, max_len=struct_max_len)\n","material_labels = tf.keras.utils.to_categorical(obj_df['Material'], dtype='float32')\n","weight_labels = tf.keras.utils.to_categorical(obj_df['Weight'], dtype='float32')\n","colour_labels = tf.keras.utils.to_categorical(obj_df['Colour'], dtype='float32')\n","volume_labels = tf.keras.utils.to_categorical(obj_df['Volume'], dtype='float32')\n","object_labels = tf.keras.utils.to_categorical(obj_df['Object'], dtype='float32')\n","length_labels = tf.keras.utils.to_categorical(obj_df['Length'], dtype='float32')\n","width_labels = tf.keras.utils.to_categorical(obj_df['Width'], dtype='float32')\n","height_labels = tf.keras.utils.to_categorical(obj_df['Height'], dtype='float32')\n","part_labels = np.asarray(list(obj_df['Parts']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eBA7etPn7qDI"},"source":["'''\n","Build the model, note that the function signature when using\n","multiple output model and single output model\n","'''\n","\n","model = build_model(bert_layer, weight_list, max_len=max_len)\n","# material_model = build_model(bert_layer, material_list, max_len=max_len)\n","# colour_model = build_model(bert_layer, colour_list, max_len=max_len)\n","# weight_model = build_model(bert_layer, weight_list, max_len=max_len)\n","# volume_model = build_model(bert_layer, volume_list, max_len=max_len)\n","# object_model = build_model(bert_layer, object_list, max_len=max_len)\n","# length_model = build_model(bert_layer, length_list, max_len=max_len)\n","# width_model = build_model(bert_layer, width_list, max_len=max_len)\n","# height_model = build_model(bert_layer, height_list, max_len=max_len)\n","\n","# model = build_multi_input_model(bert_layer, struct_bert_layer, struct_max_len, max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJ8dvGtDZerf"},"source":["'''\n","Need to run this block of code if using multiple output model\n","'''\n","losses = {\n","    \"material_output\": \"categorical_crossentropy\",\n","    \"colour_output\": \"categorical_crossentropy\",\n","    \"weight_output\": \"categorical_crossentropy\",\n","    'volume_output': 'categorical_crossentropy',\n","    'object_output': 'categorical_crossentropy',\n","    \"length_output\": \"categorical_crossentropy\",\n","    \"width_output\": \"categorical_crossentropy\",\n","    \"height_output\": \"categorical_crossentropy\",\n","    'part_output': 'binary_crossentropy'\n","}\n","\n","model.compile(loss=losses, optimizer=tf.keras.optimizers.Adam(lr=1e-5), metrics=['accuracy'])\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DA2jph2Y9ALN"},"source":["'''\n","Plot the model blueprint\n","'''\n","tf.keras.utils.plot_model(model, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9eDOn3G7yG7"},"source":["'''\n","Prepare data for training\n","'''\n","from sklearn.model_selection import train_test_split\n","\n","split_data = train_test_split(inputs[0], inputs[1], inputs[2], struct_inputs[0], struct_inputs[1], struct_inputs[2],\n","                              material_labels, colour_labels, weight_labels, volume_labels, object_labels, length_labels, width_labels, height_labels,\n","                              part_labels, test_size=0.20, random_state=33)\n","\n","(x_train_tokens, x_test_tokens,\n"," x_train_masks, x_test_masks,\n"," x_train_segments, x_test_segments,\n"," struct_x_train_tokens, struct_x_test_tokens,\n"," struct_x_train_masks, struct_x_test_masks,\n"," struct_x_train_segments, struct_x_test_segments,\n"," y_train_material, y_test_material,\n"," y_train_colour, y_test_colour,\n"," y_train_weight, y_test_weight,\n"," y_train_volume, y_test_volume,\n"," y_train_object, y_test_object,\n"," y_train_length, y_test_length,\n"," y_train_width, y_test_width,\n"," y_train_height, y_test_height,\n"," y_train_parts, y_test_parts) = split_data\n","\n","\n","train_input = (x_train_tokens, x_train_masks, x_train_segments)\n","test_input = (x_test_tokens, x_test_masks, x_test_segments)\n","\n","struct_train_input = (struct_x_train_tokens, struct_x_train_masks, struct_x_train_segments)\n","struct_test_input = (struct_x_test_tokens, struct_x_test_masks, struct_x_test_segments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEvIHnFn0pWq","executionInfo":{"status":"ok","timestamp":1623855738574,"user_tz":-60,"elapsed":919,"user":{"displayName":"William Lam","photoUrl":"","userId":"10037454904379648162"}}},"source":["'''\n","This is only needed when picking the optimal model, e.g. monitoring loss\n","'''\n","!rm -rf best_model\n","!mkdir best_model\n","\n","path = './best_model'\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(path, monitor='loss', save_best_only=True, verbose=1)\n"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7U1uXHl8AUV"},"source":["\n","'''\n","NOTE: Run this for single output model\n","change the output lables for predicting different property\n","'''\n","history = model.fit(x=train_input, y=y_train_weight, batch_size=32, validation_data=(test_input, y_test_weight), epochs=100, verbose=1)\n","\n","'''\n","NOTE: Comment the above and run the below code for multiple output model\n","'''\n","# history = model.fit(x=train_input,\n","#                     y={'material_output': y_train_material,\n","#                        'colour_output': y_train_colour,\n","#                        'weight_output': y_train_weight,\n","#                        'volume_output': y_train_volume,\n","#                        'object_output': y_train_object,\n","#                        'length_output' : y_train_length,\n","#                        'width_output' : y_train_width,\n","#                        'height_output' : y_train_height,\n","#                        'part_output' : y_train_parts},\n","#                     batch_size=32,\n","#                     validation_data=(test_input,\n","#                                      {'material_output': y_test_material,\n","#                                       'colour_output': y_test_colour,\n","#                                       'weight_output': y_test_weight,\n","#                                       'volume_output': y_test_volume,\n","#                                       'object_output': y_test_object,\n","#                                       'length_output' : y_test_length,\n","#                                       'width_output' : y_test_width,\n","#                                       'height_output' : y_test_height,\n","#                                       'part_output' : y_test_parts}),\n","#                     epochs=100,\n","#                     # callbacks=[checkpoint],\n","#                     verbose=1)\n","\n","'''\n","For multiple input\n","'''\n","\n","# history = model.fit(x=[train_input, struct_train_input],\n","#                     y={\"material_output\": y_train_material,\n","#                        'colour_output': y_train_colour,\n","#                        'weight_output': y_train_weight,\n","#                        'volume_output': y_train_volume,\n","#                        'object_output': y_train_object,\n","#                        'length_output' : y_train_length,\n","#                        'width_output' : y_train_width,\n","#                        'height_output' : y_train_height},\n","#                     batch_size=32,\n","#                     validation_data=([test_input, struct_test_input],\n","#                                      {\"material_output\": y_test_material,\n","#                                       'colour_output': y_test_colour,\n","#                                       'weight_output': y_test_weight,\n","#                                       'volume_output': y_test_volume,\n","#                                       'object_output': y_test_object,\n","#                                       'length_output' : y_test_length,\n","#                                       'width_output' : y_test_width,\n","#                                       'height_output' : y_test_height}),\n","#                     epochs=1,\n","#                     callbacks=[checkpoint],\n","#                     verbose=1)\n","\n","\n","# model.load_weights(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EPkTToIdbmif"},"source":["# Below section analyse the performance of numeric properties in terms of mean absolute error and its corresponding error distribution"]},{"cell_type":"code","metadata":{"id":"rFbgwR489yA5"},"source":["pred = model.predict(test_input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uV9g-VJIfHII"},"source":["idx_to_weight = {v: k for k, v in weight_to_idx.items()}\n","idx_to_volume = {v: k for k, v in volume_to_idx.items()}\n","idx_to_length = {v: k for k, v in length_to_idx.items()}\n","idx_to_width = {v: k for k, v in width_to_idx.items()}\n","idx_to_height = {v: k for k, v in height_to_idx.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEr6umwUfJHK"},"source":["# Prepare ground truth (from onehot back to actual measurement)\n","def get_numeric_true_metric(onehot, idx_dict):\n","  num_arr = []\n","  for i in onehot:\n","    num_arr.append(int(idx_dict[np.argmax(i)]))\n","  return num_arr\n","true_weight = get_numeric_true_metric(y_test_weight, idx_to_weight)\n","true_volume = get_numeric_true_metric(y_test_volume, idx_to_volume)\n","true_length = get_numeric_true_metric(y_test_length, idx_to_length)\n","true_width = get_numeric_true_metric(y_test_width, idx_to_width)\n","true_height = get_numeric_true_metric(y_test_height, idx_to_height)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"djuh1DTCfKh2"},"source":["# Now look at the predicted values\n","# Material -> Colour -> Weight -> Volume -> Object -> Length -> Width -> Height\n","# Index of the property relating to the models\n","# prop_idx = [2, 3, 5, 6, 7]\n","'''\n","NOTE: this needs to be changed if the model is single output\n","for example\n","pred_weight, pred_weight_val = pred, []\n","pred_volume, pred_volume_val = pred, []\n","pred_length, pred_length_val = pred, []\n","pred_width, pred_width_val = pred, []\n","pred_height, pred_height_val = pred, []\n","'''\n","pred_weight, pred_weight_val = pred[2], []\n","pred_volume, pred_volume_val = pred[3], []\n","pred_length, pred_length_val = pred[5], []\n","pred_width, pred_width_val = pred[6], []\n","pred_height, pred_height_val = pred[7], []\n","\n","def convert_to_number(pred, val, idx_dict):\n","  for i in range(len(pred)):\n","    val.append(int(idx_dict[np.argmax(pred[i])]))\n","convert_to_number(pred_weight, pred_weight_val, idx_to_weight)\n","convert_to_number(pred_volume, pred_volume_val, idx_to_volume)\n","convert_to_number(pred_length, pred_length_val, idx_to_length)\n","convert_to_number(pred_width, pred_width_val, idx_to_width)\n","convert_to_number(pred_height, pred_height_val, idx_to_height)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzNzU-AMfL9a"},"source":["def get_abs_error(true, pred):\n","  abs_arr = []\n","  for i in range(len(true)):\n","    abs_arr.append(abs(true[i] - pred[i]))\n","  return abs_arr\n","\n","weight_abs_error = np.asarray(get_abs_error(true_weight, pred_weight_val))\n","volume_abs_error = np.asarray(get_abs_error(true_volume, pred_volume_val))\n","length_abs_error = np.asarray(get_abs_error(true_length, pred_length_val))\n","width_abs_error = np.asarray(get_abs_error(true_width, pred_width_val))\n","height_abs_error = np.asarray(get_abs_error(true_height, pred_height_val))\n","\n","print(f'Mean absolute error of weight: {np.mean(weight_abs_error)}g')\n","print(f'Mean absolute error of volume: {np.mean(volume_abs_error)}ml')\n","print(f'Mean absolute error of length: {np.mean(length_abs_error)}cm')\n","print(f'Mean absolute error of width : {np.mean(width_abs_error)}cm')\n","print(f'Mean absolute error of height: {np.mean(height_abs_error)}cm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XtkUDlNxgfk2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLHxrqUJrIQv"},"source":["'''\n","Create a directory to save picture\n","'''\n","!mkdir experiment3_bert"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0r4mlG2SpoZt"},"source":["plt.style.use(\"ggplot\")\n","def plot_abs_error(abs_error, property):\n","  abs_error = abs_error[abs_error != 0]\n","  weights = np.ones_like(abs_error) / len(abs_error)\n","  fig = plt.figure(figsize=(10, 4), dpi=200)\n","  ax1 = fig.add_subplot(1, 1, 1)\n","  ax1.hist(abs_error, weights=weights, bins=200)\n","  ax1.set_xlabel('Absolute Error')\n","  ax1.set_ylabel('Normalized Frequency')\n","  ax1.set_title(f'Absolute error on {property} prediction')\n","  plt.savefig(f'./experiment3_bert/{property}.png')\n","\n","plot_abs_error(weight_abs_error, 'weight')\n","plot_abs_error(volume_abs_error, 'volume')\n","plot_abs_error(length_abs_error, 'length')\n","plot_abs_error(width_abs_error, 'width')\n","plot_abs_error(height_abs_error, 'height')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrs2p_sHrPEO"},"source":["!zip -r download4.zip ./experiment4_bert"],"execution_count":null,"outputs":[]}]}