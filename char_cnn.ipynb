{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"char_cnn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMElcHqi/QUsSSP1YnYtTEm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"djeVcOukX_Ya"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","mug_csv = pd.read_csv('mug.csv')\n","kettle_csv = pd.read_csv('kettle.csv')\n","bottle_csv = pd.read_csv('bottle.csv')\n","bakingtray_csv = pd.read_csv('bakingtray.csv')\n","hammer_csv = pd.read_csv('hammer.csv')\n","pan_csv = pd.read_csv('pan.csv')\n","wrench_csv = pd.read_csv('wrench.csv')\n","\n","\n","mug_csv['Object'] = ['mug'] * len(mug_csv)\n","kettle_csv['Object'] = ['kettle'] * len(kettle_csv)\n","bottle_csv['Object'] = ['bottle'] * len(bottle_csv)\n","bakingtray_csv['Object'] = ['bakingtray'] * len(bakingtray_csv)\n","hammer_csv['Object'] = ['hammer'] * len(hammer_csv)\n","pan_csv['Object'] = ['pan'] * len(pan_csv)\n","wrench_csv['Object'] = ['wrench'] * len(wrench_csv)\n","# mug_csv.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oabrIYwSXGnz"},"source":["all_df = mug_csv.append(kettle_csv, ignore_index=True)\n","all_df = all_df.append(bottle_csv, ignore_index=True)\n","all_df = all_df.append(bakingtray_csv, ignore_index=True)\n","all_df = all_df.append(hammer_csv, ignore_index=True)\n","all_df = all_df.append(pan_csv, ignore_index=True)\n","all_df = all_df.append(wrench_csv, ignore_index=True)\n","\n","print(len(all_df))\n","all_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Bkp0rl8ZlrE"},"source":["import string # for preprocess_text()\n","import re\n","# NLTK library to remove the stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","'''\n","Want to try to method\n","1) change the digit to word\n","2) split the number into single digit\n","'''\n","num_to_word = {'1' : ' one ',\n","               '2' : ' two ',\n","               '3' : ' three ',\n","               '4' : ' four ',\n","               '5' : ' five ',\n","               '6' : ' six ',\n","               '7' : ' eight ',\n","               '8' : ' eight ',\n","               '9' : ' nine ',\n","               '0' : ' zero '}\n","\n","regex = '([0-9]+\\.?[0-9]+)([a-zA-Z]+)'\n","r = re.compile(regex)\n","\n","# unit conversion\n","convert_vol = {'oz'     : 29.5735,\n","               'cl'     : 10.0,\n","               'gallon' : 4546.09,\n","               'L'      : 1000.0}\n","char_set = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n","\n","class InputPreprocess():\n","  @staticmethod\n","  def quantize(val, to_values):\n","      \"\"\"Quantize a value with regards to a set of allowed values.\n","      \n","      Examples:\n","          quantize(49.513, [0, 45, 90]) -> 45\n","          quantize(43, [0, 10, 20, 30]) -> 30\n","      \n","      Note: function doesn't assume to_values to be sorted and\n","      iterates over all values (i.e. is rather slow).\n","      \n","      Args:\n","          val        The value to quantize\n","          to_values  The allowed values\n","      Returns:\n","          Closest value among allowed values.\n","      \"\"\"\n","      best_match = None\n","      best_match_diff = None\n","      for other_val in to_values:\n","          diff = abs(other_val - val)\n","          if best_match is None or diff <= best_match_diff:\n","              best_match = other_val\n","              best_match_diff = diff\n","      return best_match\n","\n","  # Preprocess weights --> e.g. 990g -> 990 <int>\n","  @staticmethod\n","  def preprocess_weights(df):\n","    tmp = df.copy()\n","    weights = tmp['Weight'].to_list()\n","    # Want to extract rows with 'g' only\n","    row_g = []\n","    for i in range(len(weights)):\n","      curr_weight = weights[i]\n","      if curr_weight.endswith('kg'):\n","        curr_weight = curr_weight[0:-2]\n","        curr_weight = float(curr_weight) * 1000\n","        # row_g.append(i)\n","      elif curr_weight.endswith('g'):\n","        curr_weight = curr_weight[0:-1]\n","        curr_weight = float(curr_weight)\n","        row_g.append(i)\n","      elif curr_weight.endswith('pounds'):\n","        curr_weight = curr_weight[0:-6]\n","        curr_weight = float(curr_weight) * 453.592\n","      elif curr_weight.endswith('oz'):\n","        curr_weight = curr_weight[0:-2]\n","        curr_weight = float(curr_weight) * 28.3495\n","      elif curr_weight == 'other':\n","        curr_weight = 0\n","      if curr_weight > 5000:\n","        curr_weight = 0\n","      weights[i] = int(curr_weight)\n","    tmp['Weight'] = weights\n","    return tmp, row_g\n","  \n","  @staticmethod\n","  def preprocess_volume(df):\n","    tmp = df.copy()\n","    volume = tmp['Volume'].to_list()\n","    for i in range(len(volume)):\n","      curr_volume = volume[i].split(';')[0] # now only consider to first element in the labels\n","\n","      if curr_volume.endswith('ml'):\n","        curr_volume = float(curr_volume[0:-2])\n","      elif curr_volume.endswith('oz'):\n","        curr_volume = float(curr_volume[0:-2]) * convert_vol['oz']\n","      elif curr_volume.endswith('g'):\n","        curr_volume = float(curr_volume[0:-1])\n","      elif curr_volume.endswith('cl'):\n","        curr_volume = float(curr_volume[0:-2]) * convert_vol['cl']\n","      elif curr_volume.endswith('L'):\n","        curr_volume = float(curr_volume[0:-1]) * convert_vol['L']\n","      elif curr_volume.endswith('gallon'):\n","        curr_volume = float(curr_volume[0:-6]) * convert_vol['gallon']\n","      elif curr_volume == 'other':\n","        curr_volume = 0\n","      volume[i] = int(curr_volume)\n","      # if volume[i] > 5000:\n","      #   volume[i] = 0\n","    tmp['Volume'] = volume\n","    return tmp\n","\n","  @staticmethod\n","  def convert_num2words(word_list):\n","    tmp_word_list = []\n","    for word in word_list:\n","      if word[0].isdigit() and word[-1].isdigit():\n","        try:\n","          num_in_word = num2words(float(word))\n","          tmp_word_list.append(num_in_word)\n","        except ValueError as e:\n","          tmp_word_list.append(word)\n","      else:\n","        tmp_word_list.append(word)\n","    return tmp_word_list\n","  @staticmethod\n","  def separate_num_unit(word_list):\n","    tmp_word_list = []\n","    for word in word_list:\n","      if word == '':\n","        continue\n","      m = r.match(word)\n","      if m is not None:\n","        tmp_word_list.extend(list(m.groups()))\n","      else:\n","        tmp_word_list.append(word)\n","    return tmp_word_list\n","\n","  @staticmethod\n","  def remove_stopwords(text):\n","    words = [w for w in text if w not in stopwords.words('english')]\n","    return words\n","  \n","  @staticmethod\n","  def remove_out_of_char(sentence):\n","    res = ''\n","    for ch in sentence:\n","      if ch in char_set:\n","        res += ch\n","      else:\n","        res += ' '\n","    return res\n","  \"\"\"\n","  Clean the text with the following rules\n","  - Convert newline \\n to white space\n","  - Convert tab \\t to white space\n","  - Lowercase all texts\n","  - Covert punctuation to white space\n","\n","  \"\"\"\n","  @staticmethod\n","  def preprocess_text(df, column):\n","    tmp = df.copy()\n","    for i in range(len(tmp)):\n","      s = tmp[column][i]\n","      s = s.lower()\n","      s = s.replace('\\n', ' ') \n","      s = s.replace('\\t', ' ')\n","      s = InputPreprocess.remove_out_of_char(s)\n","      word_list = s.split(' ')\n","      # handle cases like 12oz -> 12 oz (separate the value and the unit)\n","      tmp_word_list = InputPreprocess.separate_num_unit(word_list)\n","\n","      # word_list = InputPreprocess.convert_num2words(tmp_word_list)\n","      s = ' '.join(tmp_word_list)\n","      res_string = ''\n","      for j in range(len(s)):\n","        if s[j] == '.' and j-1 > 0 and j+1 < len(s) and s[j-1].isdigit() and s[j+1].isdigit():\n","          res_string += s[j]\n","        elif s[j] in string.punctuation:\n","          continue\n","        else:\n","          res_string += s[j]\n","\n","      # s = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in s]).split())\n","      tmp.at[i, column] = res_string.split(' ')\n","    # Remove stopwords using NLTK library\n","    print('applying NLTK')\n","    tmp[column] = tmp[column].apply(lambda x: InputPreprocess.remove_stopwords(x))\n","    for i in range(len(tmp)):\n","      s = tmp[column][i]\n","      s = ' '.join(s)\n","      tmp.at[i, column] = s\n","    return tmp\n","  \n","  @staticmethod\n","  def preprocess_dimensions(df):\n","    tmp = df.copy()\n","    length, width, height = tmp['Length'], tmp['Width'], tmp['Height']\n","    res_length, res_width, res_height = [], [], []\n","    for i in range(len(df)):\n","      if length[i] == 'other':\n","        res_length.append(0)\n","      else:\n","        res_length.append(int(round(float(length[i]))))\n","\n","      if width[i] == 'other':\n","        res_width.append(0)\n","      else:\n","        res_width.append(int(round(float(width[i]))))\n","\n","      if height[i] == 'other':\n","        res_height.append(0)\n","      else:\n","        res_height.append(int(round(float(height[i]))))\n","    tmp['Length'], tmp['Width'], tmp['Height'] = res_length, res_width, res_height\n","    return tmp\n","\n","  @staticmethod\n","  def parts_into_list(df):\n","    tmp = df.copy()\n","    for i in range(len(tmp)):\n","      s = tmp['Parts'][i]\n","      s = s.split(';')\n","      tmp.at[i, 'Parts'] = s\n","    return tmp    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YgTe84JUa80Z"},"source":["'''\n","NOTE: Change this if needed for numeric label resolution\n","'''\n","weight_vol_res = 100\n","size_res = 3\n","\n","# Pre-process on input text\n","obj_df = InputPreprocess.preprocess_text(all_df, 'Input')\n","obj_df = InputPreprocess.preprocess_text(obj_df, 'UnstructuredText')\n","obj_df = InputPreprocess.preprocess_text(obj_df, 'StructuredText')\n","\n","# Pre-process on weight and volume\n","obj_df, row_g = InputPreprocess.preprocess_weights(obj_df)\n","obj_df = InputPreprocess.preprocess_volume(obj_df)\n","weight_bins = np.arange(min(obj_df['Weight']), max(obj_df['Weight']), weight_vol_res)\n","volume_bins = np.arange(min(obj_df['Volume']), max(obj_df['Volume']), weight_vol_res)\n","obj_df['Weight'] = obj_df['Weight'].apply(lambda x : InputPreprocess.quantize(x, weight_bins))\n","obj_df['Volume'] = obj_df['Volume'].apply(lambda x : InputPreprocess.quantize(x, volume_bins))\n","\n","# Pre-process on the size\n","obj_df = InputPreprocess.preprocess_dimensions(obj_df)\n","length_bins = np.arange(min(obj_df['Length']), max(obj_df['Length']), size_res)\n","width_bins = np.arange(min(obj_df['Width']), max(obj_df['Width']), size_res)\n","height_bins = np.arange(min(obj_df['Height']), max(obj_df['Height']), size_res)\n","obj_df['Length'] = obj_df['Length'].apply(lambda x : InputPreprocess.quantize(x, length_bins))\n","obj_df['Width'] = obj_df['Width'].apply(lambda x : InputPreprocess.quantize(x, width_bins))\n","obj_df['Height'] = obj_df['Height'].apply(lambda x : InputPreprocess.quantize(x, height_bins))\n","\n","# Pre-process on the object parts\n","obj_df = InputPreprocess.parts_into_list(obj_df)\n","obj_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pl5ciYoLvAdO"},"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","mlb = MultiLabelBinarizer()\n","obj_df['Parts'] = list(mlb.fit_transform(list(obj_df['Parts'])))\n","obj_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qhqQvchXMPC"},"source":["fig = plt.figure(figsize=(5, 3), dpi=200)\n","ax1 = fig.add_subplot(1, 1, 1)\n","ax1.hist(obj_df['Weight'], bins=50)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oYDtPEh9WjT"},"source":["text_lens = []\n","for t in obj_df['Input']:\n","  text_lens.append(len(t))\n","maximum_text_length = max(text_lens)\n","print('Maximum text length: {}'.format(maximum_text_length))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94imgaIEKWxf"},"source":["material_list = list(set(obj_df[\"Material\"]))\n","print(len(material_list), material_list)\n","\n","colour_list = list(set(obj_df['Colour']))\n","print(len(colour_list), colour_list)\n","\n","weight_list = list(set(obj_df['Weight']))\n","print(len(weight_list), sorted(weight_list))\n","\n","volume_list = list(set(obj_df['Volume']))\n","print(len(volume_list), sorted(volume_list))\n","\n","object_list = list(set(obj_df['Object']))\n","print(len(object_list), sorted(object_list))\n","\n","length_list = list(set(obj_df['Length']))\n","print(len(length_list), sorted(length_list))\n","\n","width_list = list(set(obj_df['Width']))\n","print(len(width_list), sorted(width_list))\n","\n","height_list = list(set(obj_df['Height']))\n","print(len(height_list), sorted(height_list))\n","\n","part_list = list(mlb.classes_)\n","print(part_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yh11KscIEFM9"},"source":["# Create the dictionary for material and colour\n","material_to_idx = {x : i for i, x in enumerate(material_list)}\n","colour_to_idx = {x : i for i, x in enumerate(colour_list)}\n","weight_to_idx = {x : i for i, x in enumerate(weight_list)}\n","volume_to_idx = {x : i for i, x in enumerate(volume_list)}\n","object_to_idx = {x : i for i, x in enumerate(object_list)}\n","length_to_idx = {x : i for i, x in enumerate(length_list)}\n","width_to_idx = {x : i for i, x in enumerate(width_list)}\n","height_to_idx = {x : i for i, x in enumerate(height_list)}\n","# Now update the label to integer form\n","for i in range(len(obj_df)):\n","  obj_df.iloc[i, 3] = material_to_idx[obj_df.iloc[i, 3]]\n","  obj_df.iloc[i, 4] = colour_to_idx[obj_df.iloc[i, 4]]\n","  obj_df.iloc[i, 5] = weight_to_idx[obj_df.iloc[i, 5]]\n","  obj_df.iloc[i, 6] = volume_to_idx[obj_df.iloc[i, 6]]\n","  obj_df.iloc[i, 7] = length_to_idx[obj_df.iloc[i, 7]]\n","  obj_df.iloc[i, 8] = width_to_idx[obj_df.iloc[i, 8]]\n","  obj_df.iloc[i, 9] = height_to_idx[obj_df.iloc[i, 9]]\n","  obj_df.iloc[i, -1] = object_to_idx[obj_df.iloc[i, -1]]\n","obj_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZF5kRc1hKdJz"},"source":["text_preprocess_df = obj_df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-VPCeDIbREc"},"source":["# Print the number of output label classes\n","print('Material: {} \\t Colour: {} \\t Weight: {}'.format(len(material_to_idx), len(colour_to_idx), len(weight_to_idx)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dK8O7l5taLjW"},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf\n","\n","tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n","tk.fit_on_texts(text_preprocess_df['Input'].values)\n","\n","alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n","char_dict = {}\n","for i, char in enumerate(alphabet):\n","    char_dict[char] = i + 1\n","    \n","# Use char_dict to replace the tk.word_index\n","tk.word_index = char_dict.copy() \n","# Add 'UNK' to the vocabulary \n","tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n","\n","train_sequences = tk.texts_to_sequences(text_preprocess_df['Input'].values)\n","train_data = pad_sequences(train_sequences, maxlen=maximum_text_length, padding='pre')\n","\n","# Convert to numpy array\n","# train_data = np.array(train_data, dtype='float32')\n","\n","# For output labels\n","material_labels = tf.keras.utils.to_categorical(np.asarray(text_preprocess_df['Material']))\n","colour_labels = tf.keras.utils.to_categorical(np.asarray(text_preprocess_df['Colour']))\n","weight_labels = tf.keras.utils.to_categorical(np.asarray(text_preprocess_df['Weight']))\n","volume_labels = tf.keras.utils.to_categorical(np.asarray(text_preprocess_df['Volume']))\n","object_labels = tf.keras.utils.to_categorical(np.asarray(text_preprocess_df['Object']))\n","length_labels = tf.keras.utils.to_categorical(np.asarray(text_preprocess_df['Length']))\n","width_labels = tf.keras.utils.to_categorical(np.asarray(text_preprocess_df['Width']))\n","height_labels = tf.keras.utils.to_categorical(np.asarray(text_preprocess_df['Height']))\n","part_labels = np.asarray(list(obj_df['Parts']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88Rn7Yvqapvr"},"source":["vocab_size = len(tk.word_index)\n","vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CDf-_bvVdJb0"},"source":["embedding_weights = []\n","embedding_weights.append(np.zeros(vocab_size)) # zero vector to represent the PAD\n","\n","for char, i in tk.word_index.items():\n","  onehot = np.zeros(vocab_size)\n","  onehot[i-1] = 1\n","  embedding_weights.append(onehot)\n","embedding_weights = np.array(embedding_weights)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vff2xoaYdmjq"},"source":["# First row all 0 for PAD, 68 char, last row for UNK\n","print(embedding_weights.shape)\n","print(embedding_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zGTeWgVdP7c"},"source":["# Parameter\n","input_size = maximum_text_length\n","embedding_size = 69\n","# filter_num, filter_size, pooling_size\n","conv_layers = [[256, 7, 3],\n","               [256, 7, 3],\n","               [256, 3, -1],\n","               [256, 3, -1],\n","               [256, 3, -1],\n","               [256, 3, 3]]\n","fully_connected_layers = [1024, 1024]\n","num_of_material_classes = len(material_to_idx)\n","num_of_colour_classes = len(colour_to_idx)\n","num_of_weight_classes = len(weight_to_idx)\n","num_of_volume_classes = len(volume_to_idx)\n","num_of_object_classes = len(object_to_idx)\n","num_of_length_classes = len(length_to_idx)\n","num_of_width_classes = len(width_to_idx)\n","num_of_height_classes = len(height_to_idx)\n","num_of_part_classes = len(part_list)\n","dropout_p = 0.5\n","optimizer = 'adam'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFELZEWRdkdW"},"source":["from keras.layers import Input, Embedding, Activation, Flatten, Dense\n","from keras.layers import Conv1D, MaxPooling1D, Dropout\n","from keras.models import Model\n","\n","# Embedding layer Initialization\n","embedding_layer = Embedding(vocab_size + 1,\n","                            embedding_size,\n","                            input_length=input_size,\n","                            weights=[embedding_weights])\n","\n","inputs = Input(shape=(input_size,), name='input', dtype='int64')\n","\n","x = embedding_layer(inputs)\n","# Convolution layers\n","for filter_num, filter_size, pooling_size in conv_layers:\n","  x = Conv1D(filter_num, filter_size)(x)\n","  x = Activation('relu')(x)\n","  if pooling_size != -1:\n","    x = MaxPooling1D(pool_size=pooling_size)(x)\n","\n","x = Flatten()(x)\n","\n","for dense_size in fully_connected_layers:\n","  x = Dense(dense_size, activation='relu')(x)\n","  x = Dropout(dropout_p)(x)\n","\n","material_branch = Dense(num_of_material_classes, activation='softmax', name='material_output')(x)\n","colour_branch = Dense(num_of_colour_classes, activation='softmax', name='colour_output')(x)\n","weight_branch = Dense(num_of_weight_classes, activation='softmax', name='weight_output')(x)\n","volume_branch = Dense(num_of_volume_classes, activation='softmax', name='volume_output')(x)\n","object_branch = Dense(num_of_object_classes, activation='softmax', name='object_output')(x)\n","length_branch = Dense(num_of_length_classes, activation='softmax', name='length_output')(x)\n","width_branch = Dense(num_of_width_classes, activation='softmax', name='width_output')(x)\n","height_branch = Dense(num_of_height_classes, activation='softmax', name='height_output')(x)\n","part_branch = Dense(num_of_part_classes, activation='sigmoid', name='part_output')(x)\n","\n","\n","\n","\n","\n","\n","\n","# Build model\n","'''\n","Multiple output\n","'''\n","model = Model(inputs=inputs,  outputs=[material_branch,\n","                                       colour_branch,\n","                                       weight_branch,\n","                                       volume_branch,\n","                                       object_branch,\n","                                       length_branch,\n","                                       width_branch,\n","                                       height_branch,\n","                                       part_branch])\n","'''\n","Single output\n","'''\n","\n","# outputs = part_branch\n","# model = Model(inputs=inputs, outputs=outputs)\n","\n","'''\n","Change this for multiple or single output\n","'''\n","losses = {\n","    \"material_output\": \"categorical_crossentropy\",\n","    \"colour_output\": \"categorical_crossentropy\",\n","    \"weight_output\": \"categorical_crossentropy\",\n","    \"volume_output\": \"categorical_crossentropy\",\n","    \"object_output\": \"categorical_crossentropy\",\n","    \"length_output\": \"categorical_crossentropy\",\n","    \"width_output\": \"categorical_crossentropy\",\n","    \"height_output\": \"categorical_crossentropy\",\n","    'part_output' : 'binary_crossentropy'\n","}\n","# loss = 'binary_crossentropy'\n","model.compile(optimizer=optimizer, loss=losses, metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cx8Ei_QBjAdK"},"source":["'''\n","Plot model\n","'''\n","tf.keras.utils.plot_model(model, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5m_GhPK1mn4"},"source":["'''\n","Prepare data\n","'''\n","from sklearn.model_selection import train_test_split\n","\n","# Model training params\n","# new_input_list or data1 variable\n","split_data = train_test_split(train_data,\n","                              material_labels,\n","                              colour_labels,\n","                              weight_labels,\n","                              volume_labels,\n","                              object_labels,\n","                              length_labels,\n","                              width_labels,\n","                              height_labels,\n","                              part_labels,\n","                              test_size=0.20, random_state=33)\n","(x_train, x_test,\n"," y_train_material, y_test_material,\n"," y_train_colour, y_test_colour,\n"," y_train_weight, y_test_weight,\n"," y_train_volume, y_test_volume,\n"," y_train_object, y_test_object,\n"," y_train_length, y_test_length,\n"," y_train_width, y_test_width,\n"," y_train_height, y_test_height,\n"," y_train_parts, y_test_parts) = split_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sf4DE8S2ggwS"},"source":["# Set up directory to save best model\n","!rm -rf best_model\n","!mkdir best_model\n","\n","path = './best_model'\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(path, monitor='val_accuracy', save_best_only=True, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgMylZ87egLl"},"source":["\n","epochs = 100\n","# model.fit(x_train, y_train_parts, validation_data=(x_test, y_test_parts), batch_size=32, epochs=epochs, verbose=1)\n","\n","history = model.fit(x=x_train,\n","                    y={\"material_output\": y_train_material,\n","                       \"colour_output\": y_train_colour,\n","                       'weight_output': y_train_weight,\n","                       'volume_output' : y_train_volume,\n","                       'object_output' : y_train_object,\n","                       'length_output' : y_train_length,\n","                       'width_output' : y_train_width,\n","                       'height_output' : y_train_height,\n","                       'part_output' : y_train_parts},\n","                    batch_size=32,\n","                    validation_data=(x_test,\n","                                     {\"material_output\": y_test_material,\n","                                      \"colour_output\": y_test_colour,\n","                                      'weight_output': y_test_weight,\n","                                      'volume_output' : y_test_volume,\n","                                      'object_output' : y_test_object,\n","                                      'length_output' : y_test_length,\n","                                      'width_output' : y_test_width,\n","                                      'height_output' : y_test_height,\n","                                      'part_output' : y_test_parts}),\n","                    # callbacks=[checkpoint],\n","                    epochs=epochs,\n","                    verbose=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xwFsOXc0eduE"},"source":["'''\n"," Load weight if use best model only \n","'''\n","# model.load_weights(path)\n","pred = model.predict(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4gVah6uPz63"},"source":["idx_to_weight = {v: k for k, v in weight_to_idx.items()}\n","idx_to_volume = {v: k for k, v in volume_to_idx.items()}\n","idx_to_length = {v: k for k, v in length_to_idx.items()}\n","idx_to_width = {v: k for k, v in width_to_idx.items()}\n","idx_to_height = {v: k for k, v in height_to_idx.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EH3HdP22g8bh"},"source":["# Prepare ground truth (from onehot back to actual measurement)\n","def get_numeric_true_metric(onehot, idx_dict):\n","  num_arr = []\n","  for i in onehot:\n","    num_arr.append(int(idx_dict[np.argmax(i)]))\n","  return num_arr\n","true_weight = get_numeric_true_metric(y_test_weight, idx_to_weight)\n","true_volume = get_numeric_true_metric(y_test_volume, idx_to_volume)\n","true_length = get_numeric_true_metric(y_test_length, idx_to_length)\n","true_width = get_numeric_true_metric(y_test_width, idx_to_width)\n","true_height = get_numeric_true_metric(y_test_height, idx_to_height)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfEkyDPEg87m"},"source":["# Now look at the predicted values\n","# Material -> Colour -> Weight -> Volume -> Object -> Length -> Width -> Height\n","# Index of the property relating to the models\n","# prop_idx = [2, 3, 5, 6, 7]\n","'''\n","NOTE: this needs to be changed if the model is single output\n","for example\n","pred_weight, pred_weight_val = pred, []\n","pred_volume, pred_volume_val = pred, []\n","pred_length, pred_length_val = pred, []\n","pred_width, pred_width_val = pred, []\n","pred_height, pred_height_val = pred, []\n","'''\n","pred_weight, pred_weight_val = pred[2], []\n","pred_volume, pred_volume_val = pred[3], []\n","pred_length, pred_length_val = pred[5], []\n","pred_width, pred_width_val = pred[6], []\n","pred_height, pred_height_val = pred[7], []\n","\n","def convert_to_number(pred, val, idx_dict):\n","  for i in range(len(pred)):\n","    val.append(int(idx_dict[np.argmax(pred[i])]))\n","convert_to_number(pred_weight, pred_weight_val, idx_to_weight)\n","convert_to_number(pred_volume, pred_volume_val, idx_to_volume)\n","convert_to_number(pred_length, pred_length_val, idx_to_length)\n","convert_to_number(pred_width, pred_width_val, idx_to_width)\n","convert_to_number(pred_height, pred_height_val, idx_to_height)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqWpeTIDg_ZN"},"source":["def get_abs_error(true, pred):\n","  abs_arr = []\n","  for i in range(len(true)):\n","    abs_arr.append(abs(true[i] - pred[i]))\n","  return abs_arr\n","\n","weight_abs_error = np.asarray(get_abs_error(true_weight, pred_weight_val))\n","volume_abs_error = np.asarray(get_abs_error(true_volume, pred_volume_val))\n","length_abs_error = np.asarray(get_abs_error(true_length, pred_length_val))\n","width_abs_error = np.asarray(get_abs_error(true_width, pred_width_val))\n","height_abs_error = np.asarray(get_abs_error(true_height, pred_height_val))\n","\n","print(f'Mean absolute error of weight: {np.mean(weight_abs_error)}g')\n","print(f'Mean absolute error of volume: {np.mean(volume_abs_error)}ml')\n","print(f'Mean absolute error of length: {np.mean(length_abs_error)}cm')\n","print(f'Mean absolute error of width : {np.mean(width_abs_error)}cm')\n","print(f'Mean absolute error of height: {np.mean(height_abs_error)}cm')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jg8YrWrLhC1P"},"source":["\n","!mkdir ./experiment4_cnn/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIO2r2_miQ-v"},"source":["plt.style.use(\"ggplot\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76T23ihohDGF"},"source":["def plot_abs_error(abs_error, property):\n","  abs_error = abs_error[abs_error != 0]\n","  weights = np.ones_like(abs_error) / len(abs_error)\n","  fig = plt.figure(figsize=(10, 4), dpi=200)\n","  ax1 = fig.add_subplot(1, 1, 1)\n","  ax1.hist(abs_error, weights=weights, bins=200)\n","  ax1.set_xlabel('Absolute Error')\n","  ax1.set_ylabel('Normalized Frequency')\n","  ax1.set_title(f'Absolute error on {property} prediction')\n","  plt.savefig(f'./experiment4_cnn/{property}.png')\n","\n","plot_abs_error(weight_abs_error, 'weight')\n","plot_abs_error(volume_abs_error, 'volume')\n","plot_abs_error(length_abs_error, 'length')\n","plot_abs_error(width_abs_error, 'width')\n","plot_abs_error(height_abs_error, 'height')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ns3no7IBiIhy"},"source":["!zip -r download4.zip ./experiment4_cnn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tpGm8GMNk1Ey"},"source":[""],"execution_count":null,"outputs":[]}]}